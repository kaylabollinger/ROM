%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{tgtermes}
\usepackage{tgheros}
\renewcommand{\ttdefault}{txtt}



\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=auto}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{1}



\title{ROM}
\date{Apr 10, 2022}
\release{0.0.1}
\author{Kayla Bollinger}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


\sphinxAtStartPar
ROM is a reduced order model regression package with implementations of the ROMs introduced in Bollinger (2022).


\chapter{API Documentation}
\label{\detokenize{rom:api-documentation}}\label{\detokenize{rom::doc}}

\section{rom.gradients module}
\label{\detokenize{rom:module-rom.gradients}}\label{\detokenize{rom:rom-gradients-module}}\index{module@\spxentry{module}!rom.gradients@\spxentry{rom.gradients}}\index{rom.gradients@\spxentry{rom.gradients}!module@\spxentry{module}}\index{local\_linear\_gradients() (in module rom.gradients)@\spxentry{local\_linear\_gradients()}\spxextra{in module rom.gradients}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.gradients.local_linear_gradients}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{rom.gradients.}}\sphinxbfcode{\sphinxupquote{local\_linear\_gradients}}}{\emph{\DUrole{n}{X}}, \emph{\DUrole{n}{Y}}, \emph{\DUrole{n}{n}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\sphinxAtStartPar
Estimates gradients via local linear approximations.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{X}} (\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} M\sphinxhyphen{}by\sphinxhyphen{}d 2D numpy array containing input training samples

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{Y}} (\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} M length 1D numpy array containing the 1D output training samples

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{n}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} number of nearest neighbors to use for local linear approximation

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
M\sphinxhyphen{}by\sphinxhyphen{}d 2D numpy array containing local linear approximation of gradients for each training sample

\item[{Return type}] \leavevmode
\sphinxAtStartPar
DY (ndarray)

\end{description}\end{quote}

\end{fulllineitems}



\section{rom.response\_surfaces module}
\label{\detokenize{rom:module-rom.response_surfaces}}\label{\detokenize{rom:rom-response-surfaces-module}}\index{module@\spxentry{module}!rom.response\_surfaces@\spxentry{rom.response\_surfaces}}\index{rom.response\_surfaces@\spxentry{rom.response\_surfaces}!module@\spxentry{module}}\index{NN\_alt (class in rom.response\_surfaces)@\spxentry{NN\_alt}\spxextra{class in rom.response\_surfaces}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.NN_alt}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{rom.response\_surfaces.}}\sphinxbfcode{\sphinxupquote{NN\_alt}}}{\emph{\DUrole{n}{U}}, \emph{\DUrole{n}{dim\_layers}}, \emph{\DUrole{n}{lr}\DUrole{o}{=}\DUrole{default_value}{0.001}}, \emph{\DUrole{n}{lr\_decay}\DUrole{o}{=}\DUrole{default_value}{0.9}}, \emph{\DUrole{n}{alpha}\DUrole{o}{=}\DUrole{default_value}{0.001}}}{}
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{object}}

\sphinxAtStartPar
Shallow ReLU Network with alternating minimization surrogate model.
\index{net (rom.response\_surfaces.NN\_alt attribute)@\spxentry{net}\spxextra{rom.response\_surfaces.NN\_alt attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.NN_alt.net}}\pysigline{\sphinxbfcode{\sphinxupquote{net}}}
\sphinxAtStartPar
two layer ReLU network

\end{fulllineitems}

\index{optimizer (rom.response\_surfaces.NN\_alt attribute)@\spxentry{optimizer}\spxextra{rom.response\_surfaces.NN\_alt attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.NN_alt.optimizer}}\pysigline{\sphinxbfcode{\sphinxupquote{optimizer}}}
\sphinxAtStartPar
optimizer for network

\end{fulllineitems}

\index{loss\_func (rom.response\_surfaces.NN\_alt attribute)@\spxentry{loss\_func}\spxextra{rom.response\_surfaces.NN\_alt attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.NN_alt.loss_func}}\pysigline{\sphinxbfcode{\sphinxupquote{loss\_func}}}
\sphinxAtStartPar
loss function for network

\end{fulllineitems}

\index{lr (rom.response\_surfaces.NN\_alt attribute)@\spxentry{lr}\spxextra{rom.response\_surfaces.NN\_alt attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.NN_alt.lr}}\pysigline{\sphinxbfcode{\sphinxupquote{lr}}}
\sphinxAtStartPar
learning rate
\begin{quote}\begin{description}
\item[{Type}] \leavevmode
\sphinxAtStartPar
float

\end{description}\end{quote}

\end{fulllineitems}

\index{lr\_decay (rom.response\_surfaces.NN\_alt attribute)@\spxentry{lr\_decay}\spxextra{rom.response\_surfaces.NN\_alt attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.NN_alt.lr_decay}}\pysigline{\sphinxbfcode{\sphinxupquote{lr\_decay}}}
\sphinxAtStartPar
learning rate decay
\begin{quote}\begin{description}
\item[{Type}] \leavevmode
\sphinxAtStartPar
float

\end{description}\end{quote}

\end{fulllineitems}

\index{alpha (rom.response\_surfaces.NN\_alt attribute)@\spxentry{alpha}\spxextra{rom.response\_surfaces.NN\_alt attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.NN_alt.alpha}}\pysigline{\sphinxbfcode{\sphinxupquote{alpha}}}
\sphinxAtStartPar
regularization parameter (ell 2 regularizer)
\begin{quote}\begin{description}
\item[{Type}] \leavevmode
\sphinxAtStartPar
float

\end{description}\end{quote}

\end{fulllineitems}

\index{U (rom.response\_surfaces.NN\_alt attribute)@\spxentry{U}\spxextra{rom.response\_surfaces.NN\_alt attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.NN_alt.U}}\pysigline{\sphinxbfcode{\sphinxupquote{U}}}
\sphinxAtStartPar
d\sphinxhyphen{}by\sphinxhyphen{}k 2D numpy array to reduce input dimension
\begin{quote}\begin{description}
\item[{Type}] \leavevmode
\sphinxAtStartPar
ndarray

\end{description}\end{quote}

\end{fulllineitems}

\index{loss\_train (rom.response\_surfaces.NN\_alt attribute)@\spxentry{loss\_train}\spxextra{rom.response\_surfaces.NN\_alt attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.NN_alt.loss_train}}\pysigline{\sphinxbfcode{\sphinxupquote{loss\_train}}}
\sphinxAtStartPar
contains training loss values at each epoch
\begin{quote}\begin{description}
\item[{Type}] \leavevmode
\sphinxAtStartPar
list

\end{description}\end{quote}

\end{fulllineitems}

\index{loss\_val (rom.response\_surfaces.NN\_alt attribute)@\spxentry{loss\_val}\spxextra{rom.response\_surfaces.NN\_alt attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.NN_alt.loss_val}}\pysigline{\sphinxbfcode{\sphinxupquote{loss\_val}}}
\sphinxAtStartPar
contains validation loss values at each epoch
\begin{quote}\begin{description}
\item[{Type}] \leavevmode
\sphinxAtStartPar
list

\end{description}\end{quote}

\end{fulllineitems}

\index{predict() (rom.response\_surfaces.NN\_alt method)@\spxentry{predict()}\spxextra{rom.response\_surfaces.NN\_alt method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.NN_alt.predict}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{predict}}}{\emph{\DUrole{n}{X}}}{}
\sphinxAtStartPar
Calculates output of trained NN model.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{X}} (\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} ?\sphinxhyphen{}by\sphinxhyphen{}d 2D numpy array containing input data

\item[{Returns}] \leavevmode
\sphinxAtStartPar
?\sphinxhyphen{}by\sphinxhyphen{}d2 2D numpy array containing output data

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Y (ndarray)

\end{description}\end{quote}

\end{fulllineitems}

\index{rel\_error\_tensor() (rom.response\_surfaces.NN\_alt method)@\spxentry{rel\_error\_tensor()}\spxextra{rom.response\_surfaces.NN\_alt method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.NN_alt.rel_error_tensor}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{rel\_error\_tensor}}}{\emph{\DUrole{n}{Y\_true}}, \emph{\DUrole{n}{Y\_calc}}}{}
\sphinxAtStartPar
Calculates relative error (via ell 2 norm) between 2 PyTorch tensors.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{Y\_true}} (\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} numpy array containing true values

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{Y\_calc}} (\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} numpy array containing calculated/predicted values

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
relative error between Y\_true and Y\_calc

\item[{Return type}] \leavevmode
\sphinxAtStartPar
(float)

\end{description}\end{quote}

\end{fulllineitems}

\index{sub\_dres() (rom.response\_surfaces.NN\_alt method)@\spxentry{sub\_dres()}\spxextra{rom.response\_surfaces.NN\_alt method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.NN_alt.sub_dres}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{sub\_dres}}}{\emph{\DUrole{n}{U}}, \emph{\DUrole{n}{X\_train}}, \emph{\DUrole{n}{Y\_train}}}{}
\sphinxAtStartPar
Defines derivative of cost function w.r.t. U.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
This function is defined for the pymanopt optimization problem. Must take in U as variable.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{U}} (\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} d\sphinxhyphen{}by\sphinxhyphen{}k 2D numpy array containing reduced basis

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{X\_train}} (\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} M\sphinxhyphen{}by\sphinxhyphen{}d 2D numpy array containing input training data

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{Y\_train}} (\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} M\sphinxhyphen{}by\sphinxhyphen{}d2 2D numpy array containing output training data

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
d\sphinxhyphen{}by\sphinxhyphen{}k 2D numpy array containing reduced basis

\item[{Return type}] \leavevmode
\sphinxAtStartPar
(ndarray)

\end{description}\end{quote}

\end{fulllineitems}

\index{sub\_res() (rom.response\_surfaces.NN\_alt method)@\spxentry{sub\_res()}\spxextra{rom.response\_surfaces.NN\_alt method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.NN_alt.sub_res}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{sub\_res}}}{\emph{\DUrole{n}{U}}, \emph{\DUrole{n}{X\_train}}, \emph{\DUrole{n}{Y\_train}}}{}
\sphinxAtStartPar
Defines cost function w.r.t. U.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
This function is defined for the pymanopt optimization problem. Must take in U as variable.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{U}} (\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} d\sphinxhyphen{}by\sphinxhyphen{}k 2D numpy array containing reduced basis

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{X\_train}} (\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} M\sphinxhyphen{}by\sphinxhyphen{}d 2D numpy array containing input training data

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{Y\_train}} (\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} M\sphinxhyphen{}by\sphinxhyphen{}d2 2D numpy array containing output training data

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
cost value for given U

\item[{Return type}] \leavevmode
\sphinxAtStartPar
out (float)

\end{description}\end{quote}

\end{fulllineitems}

\index{train() (rom.response\_surfaces.NN\_alt method)@\spxentry{train()}\spxextra{rom.response\_surfaces.NN\_alt method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.NN_alt.train}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{dataset\_train}}, \emph{\DUrole{n}{dataset\_val}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{num\_outer}\DUrole{o}{=}\DUrole{default_value}{10}}, \emph{\DUrole{n}{num\_epoch}\DUrole{o}{=}\DUrole{default_value}{5000}}, \emph{\DUrole{n}{batch\_size}\DUrole{o}{=}\DUrole{default_value}{16}}, \emph{\DUrole{n}{record}\DUrole{o}{=}\DUrole{default_value}{True}}, \emph{\DUrole{n}{verbose}\DUrole{o}{=}\DUrole{default_value}{2}}}{}
\sphinxAtStartPar
Trains NN model via alternating minimization scheme.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dataset\_train}} (\sphinxstyleliteralemphasis{\sphinxupquote{tuple}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} {[}X\_train,Y\_train{]}, numpy array containing input/output training data

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dataset\_val}} (\sphinxstyleliteralemphasis{\sphinxupquote{tuple}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} {[}X\_val,Y\_val{]}, numpy array containing input/output validation data

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_outer}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} number of outer iterations to perform

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_epoch}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} number of inner iterations for NN training to perform

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{batch\_size}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} number of training samples in each training batch

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{record}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) \textendash{} choose to record loss during training or not

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{verbose}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} amount of information to print; 0 = nothing printed; 1 = loss values printed every 1000 epochs; 2 = loss values printed every 100 epochs; \textgreater{}2 = loss values printed every epoch

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{train\_NN() (rom.response\_surfaces.NN\_alt method)@\spxentry{train\_NN()}\spxextra{rom.response\_surfaces.NN\_alt method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.NN_alt.train_NN}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train\_NN}}}{\emph{\DUrole{n}{dataset\_train}}, \emph{\DUrole{n}{dataset\_val}}, \emph{\DUrole{n}{num\_epoch}}, \emph{\DUrole{n}{batch\_size}}, \emph{\DUrole{n}{record}}, \emph{\DUrole{n}{verbose}}}{}
\sphinxAtStartPar
Trains and validates NN.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dataset\_train}} (\sphinxstyleliteralemphasis{\sphinxupquote{tuple}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} {[}X\_train,Y\_train{]}, numpy array containing input/output training data

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dataset\_val}} (\sphinxstyleliteralemphasis{\sphinxupquote{tuple}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} {[}X\_val,Y\_val{]}, numpy array containing input/output validation data

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_outer}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} number of outer iterations to perform

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_epoch}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} number of inner iterations for NN training to perform

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{batch\_size}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} number of training samples in each training batch

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{record}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) \textendash{} choose to record loss during training or not

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{verbose}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} amount of information to print; 0 = nothing printed; 1 = loss values printed every 1000 epochs; 2 = loss values printed every 100 epochs; \textgreater{}2 = loss values printed every epoch

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
contains training loss at each epoch
loss\_val (list): contains validation loss at each epoch

\item[{Return type}] \leavevmode
\sphinxAtStartPar
loss\_train (list)

\end{description}\end{quote}

\end{fulllineitems}

\index{train\_sub() (rom.response\_surfaces.NN\_alt method)@\spxentry{train\_sub()}\spxextra{rom.response\_surfaces.NN\_alt method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.NN_alt.train_sub}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train\_sub}}}{\emph{\DUrole{n}{X\_train}}, \emph{\DUrole{n}{Y\_train}}}{}
\sphinxAtStartPar
Trains subspace U.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{X\_train}} (\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} M\sphinxhyphen{}by\sphinxhyphen{}d 2D numpy array containing input training data

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{Y\_train}} (\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} M\sphinxhyphen{}by\sphinxhyphen{}d2 2D numpy array containing output training data

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{RFE (class in rom.response\_surfaces)@\spxentry{RFE}\spxextra{class in rom.response\_surfaces}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.RFE}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{rom.response\_surfaces.}}\sphinxbfcode{\sphinxupquote{RFE}}}
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{object}}

\sphinxAtStartPar
Random Feature Expansion surrogate model.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
All attributes are set after the “train” method is called.
\end{sphinxadmonition}
\index{c (rom.response\_surfaces.RFE attribute)@\spxentry{c}\spxextra{rom.response\_surfaces.RFE attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.RFE.c}}\pysigline{\sphinxbfcode{\sphinxupquote{c}}}
\sphinxAtStartPar
N length 1D numpy array containing coefficients of learned RFE model
\begin{quote}\begin{description}
\item[{Type}] \leavevmode
\sphinxAtStartPar
ndarray

\end{description}\end{quote}

\end{fulllineitems}

\index{Omega (rom.response\_surfaces.RFE attribute)@\spxentry{Omega}\spxextra{rom.response\_surfaces.RFE attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.RFE.Omega}}\pysigline{\sphinxbfcode{\sphinxupquote{Omega}}}
\sphinxAtStartPar
N\sphinxhyphen{}by\sphinxhyphen{}d 2D numpy array containing weights of the RFE model
\begin{quote}\begin{description}
\item[{Type}] \leavevmode
\sphinxAtStartPar
ndarray

\end{description}\end{quote}

\end{fulllineitems}

\index{bias (rom.response\_surfaces.RFE attribute)@\spxentry{bias}\spxextra{rom.response\_surfaces.RFE attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.RFE.bias}}\pysigline{\sphinxbfcode{\sphinxupquote{bias}}}
\sphinxAtStartPar
N length 1D numpy array containing biases of the RFE model
\begin{quote}\begin{description}
\item[{Type}] \leavevmode
\sphinxAtStartPar
ndarray

\end{description}\end{quote}

\end{fulllineitems}

\index{scale\_A\_normalize (rom.response\_surfaces.RFE attribute)@\spxentry{scale\_A\_normalize}\spxextra{rom.response\_surfaces.RFE attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.RFE.scale_A_normalize}}\pysigline{\sphinxbfcode{\sphinxupquote{scale\_A\_normalize}}}
\sphinxAtStartPar
N length 1D numpy array to normalize columns of A\_train (RF matrix with training data)
\begin{quote}\begin{description}
\item[{Type}] \leavevmode
\sphinxAtStartPar
ndarray

\end{description}\end{quote}

\end{fulllineitems}

\index{construct\_A() (rom.response\_surfaces.RFE method)@\spxentry{construct\_A()}\spxextra{rom.response\_surfaces.RFE method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.RFE.construct_A}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{construct\_A}}}{\emph{\DUrole{n}{X}}}{}
\sphinxAtStartPar
Constructs random feature matrix A.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{X}} (\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} ?\sphinxhyphen{}by\sphinxhyphen{}d 2D numpy array containing ? number of input data points

\item[{Returns}] \leavevmode
\sphinxAtStartPar
?\sphinxhyphen{}by\sphinxhyphen{}N 2D numpy array representing the random feature matrix A

\item[{Return type}] \leavevmode
\sphinxAtStartPar
A (ndarray)

\end{description}\end{quote}

\end{fulllineitems}

\index{construct\_weights() (rom.response\_surfaces.RFE method)@\spxentry{construct\_weights()}\spxextra{rom.response\_surfaces.RFE method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.RFE.construct_weights}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{construct\_weights}}}{\emph{\DUrole{n}{X\_train}}, \emph{\DUrole{n}{N}}, \emph{\DUrole{n}{k}}}{}
\sphinxAtStartPar
Constructs weights and biases for RFE model.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{X\_train}} (\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} M\sphinxhyphen{}by\sphinxhyphen{}d 2D numpy array containing input training data

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{N}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} number of weights to use for RFE model

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} dimension of input data

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{phi() (rom.response\_surfaces.RFE method)@\spxentry{phi()}\spxextra{rom.response\_surfaces.RFE method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.RFE.phi}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{phi}}}{\emph{\DUrole{n}{nodes}}}{}
\sphinxAtStartPar
Activation function phi for RFE model.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{nodes}} (\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} N length 1D numpy array containing nodal values

\item[{Returns}] \leavevmode
\sphinxAtStartPar
N length 1D numpy array containing nodes passed through activation function phi

\item[{Return type}] \leavevmode
\sphinxAtStartPar
out (ndarray)

\end{description}\end{quote}

\end{fulllineitems}

\index{predict() (rom.response\_surfaces.RFE method)@\spxentry{predict()}\spxextra{rom.response\_surfaces.RFE method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.RFE.predict}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{predict}}}{\emph{\DUrole{n}{X}}}{}
\sphinxAtStartPar
Calculates output of trained RFE model.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{X}} (\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} ?\sphinxhyphen{}by\sphinxhyphen{}d 2D numpy array containing ? number of input data points

\item[{Returns}] \leavevmode
\sphinxAtStartPar
? length 1D numpy array containing predicted output values

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Y (ndarray)

\end{description}\end{quote}

\end{fulllineitems}

\index{train() (rom.response\_surfaces.RFE method)@\spxentry{train()}\spxextra{rom.response\_surfaces.RFE method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.response_surfaces.RFE.train}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{dataset\_train}}, \emph{\DUrole{n}{N}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{alpha}\DUrole{o}{=}\DUrole{default_value}{0.001}}}{}
\sphinxAtStartPar
Trains RFE model by learning coefficients c via ridge regression.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Assumes output data is 1D.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{X\_train}} (\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} M\sphinxhyphen{}by\sphinxhyphen{}d 2D numpy array containing input training data

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{Y\_train}} (\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} M length 1D numpy array containing output training data

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{N}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} number of weights to use for RFE model

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{alpha}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} regularizing hyperparameter for ridge regression model

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\section{rom.subspaces module}
\label{\detokenize{rom:module-rom.subspaces}}\label{\detokenize{rom:rom-subspaces-module}}\index{module@\spxentry{module}!rom.subspaces@\spxentry{rom.subspaces}}\index{rom.subspaces@\spxentry{rom.subspaces}!module@\spxentry{module}}\index{AS() (in module rom.subspaces)@\spxentry{AS()}\spxextra{in module rom.subspaces}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.subspaces.AS}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{rom.subspaces.}}\sphinxbfcode{\sphinxupquote{AS}}}{\emph{\DUrole{n}{DY}}, \emph{\DUrole{n}{k}}}{}
\sphinxAtStartPar
Constructs a reduced subspace via active subspaces.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{DY}} (\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} M\sphinxhyphen{}by\sphinxhyphen{}d 2D numpy array containing gradients for each training sample

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} reduced dimension size

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
d\sphinxhyphen{}by\sphinxhyphen{}k 2D numpy array containing reduced basis vectors

\item[{Return type}] \leavevmode
\sphinxAtStartPar
U (ndarray)

\end{description}\end{quote}

\end{fulllineitems}

\index{POD() (in module rom.subspaces)@\spxentry{POD()}\spxextra{in module rom.subspaces}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.subspaces.POD}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{rom.subspaces.}}\sphinxbfcode{\sphinxupquote{POD}}}{\emph{\DUrole{n}{X}}, \emph{\DUrole{n}{k}}}{}
\sphinxAtStartPar
Constructs a reduced subspace via active subspaces.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{X}} (\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} M\sphinxhyphen{}by\sphinxhyphen{}d 2D numpy array containing gradients for each training sample

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} reduced dimension size

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
d\sphinxhyphen{}by\sphinxhyphen{}k 2D numpy array containing reduced basis vectors

\item[{Return type}] \leavevmode
\sphinxAtStartPar
U (ndarray)

\end{description}\end{quote}

\end{fulllineitems}



\section{rom.utils module}
\label{\detokenize{rom:module-rom.utils}}\label{\detokenize{rom:rom-utils-module}}\index{module@\spxentry{module}!rom.utils@\spxentry{rom.utils}}\index{rom.utils@\spxentry{rom.utils}!module@\spxentry{module}}\index{check\_1D() (in module rom.utils)@\spxentry{check\_1D()}\spxextra{in module rom.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.utils.check_1D}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{rom.utils.}}\sphinxbfcode{\sphinxupquote{check\_1D}}}{\emph{\DUrole{n}{X}}}{}
\sphinxAtStartPar
Enforces requirement that X be a 1D numpy array.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{X}} (\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} ND numpy array containing data points

\item[{Returns}] \leavevmode
\sphinxAtStartPar
unaltered 1D numpy array

\item[{Return type}] \leavevmode
\sphinxAtStartPar
X (ndarray)

\end{description}\end{quote}

\end{fulllineitems}

\index{check\_2D() (in module rom.utils)@\spxentry{check\_2D()}\spxextra{in module rom.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.utils.check_2D}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{rom.utils.}}\sphinxbfcode{\sphinxupquote{check\_2D}}}{\emph{\DUrole{n}{X}}}{}
\sphinxAtStartPar
Enforces requirement that X be a 2D numpy array.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{X}} (\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} ND numpy array containing data points

\item[{Returns}] \leavevmode
\sphinxAtStartPar
unaltered 2D numpy array

\item[{Return type}] \leavevmode
\sphinxAtStartPar
X (ndarray)

\end{description}\end{quote}

\end{fulllineitems}

\index{print\_epoch() (in module rom.utils)@\spxentry{print\_epoch()}\spxextra{in module rom.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.utils.print_epoch}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{rom.utils.}}\sphinxbfcode{\sphinxupquote{print\_epoch}}}{\emph{\DUrole{n}{verbose}}, \emph{\DUrole{n}{epoch}}, \emph{\DUrole{n}{num\_epoch}}, \emph{\DUrole{n}{loss\_train}}, \emph{\DUrole{n}{loss\_val}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{overwrite}\DUrole{o}{=}\DUrole{default_value}{False}}}{}
\sphinxAtStartPar
Prints training/validation error at given epoch.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{verbose}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} amount of information to print; 0 = nothing printed; 1 = loss values printed every 1000 epochs; 2 = loss values printed every 100 epochs; \textgreater{}2 = loss values printed every epoch

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{epoch}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} current epoch number

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_epoch}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} total epoch number

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{loss\_train}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} current loss value for training data

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{loss\_val}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} current loss value for validation data

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{overwrite}} (\sphinxstyleliteralemphasis{\sphinxupquote{boolean}}) \textendash{} choose to overwrite previous line

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{rel\_error() (in module rom.utils)@\spxentry{rel\_error()}\spxextra{in module rom.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{rom:rom.utils.rel_error}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{rom.utils.}}\sphinxbfcode{\sphinxupquote{rel\_error}}}{\emph{\DUrole{n}{Y\_true}}, \emph{\DUrole{n}{Y\_calc}}}{}
\sphinxAtStartPar
Calculates relative error (via ell 2 norm) between 2 arrays.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{Y\_true}} (\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} numpy array containing true values

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{Y\_calc}} (\sphinxstyleliteralemphasis{\sphinxupquote{ndarray}}) \textendash{} numpy array containing calculated/predicted values

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
relative error between Y\_true and Y\_calc

\item[{Return type}] \leavevmode
\sphinxAtStartPar
out (float)

\end{description}\end{quote}

\end{fulllineitems}



\chapter{License}
\label{\detokenize{license:license}}\label{\detokenize{license::doc}}
\sphinxAtStartPar
The MIT License (MIT)

\sphinxAtStartPar
Copyright (c) 2022 Kayla Bollinger

\sphinxAtStartPar
Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the “Software”), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

\sphinxAtStartPar
The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

\sphinxAtStartPar
THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


\chapter{Example:}
\label{\detokenize{index:example}}
\sphinxAtStartPar
In this example, we walk through how to train ROM’s RFE model on fluid vorticity data.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{ROM}
\PYG{k+kn}{import} \PYG{n+nn}{plotly}\PYG{n+nn}{.}\PYG{n+nn}{graph\PYGZus{}objects} \PYG{k}{as} \PYG{n+nn}{go}
\end{sphinxVerbatim}

\sphinxAtStartPar
The vorticity data can be found \sphinxhref{https://github.com/kaylabollinger/ROM}{here}.
To load the data, just replace the \sphinxcode{\sphinxupquote{data\_dir}} variable with the appropriate path to the dataset.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data\PYGZus{}dir} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data/vorticity/X.npy}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{data\PYGZus{}dir}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}
\PYG{n}{Y} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}
\end{sphinxVerbatim}

\sphinxAtStartPar
We will use the first 75 snapshots for training:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{num\PYGZus{}train} \PYG{o}{=}  \PYG{l+m+mi}{75}
\PYG{n}{X\PYGZus{}train} \PYG{o}{=} \PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{n}{num\PYGZus{}train}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}
\PYG{n}{Y\PYGZus{}train} \PYG{o}{=} \PYG{n}{Y}\PYG{p}{[}\PYG{p}{:}\PYG{n}{num\PYGZus{}train}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}
\end{sphinxVerbatim}

\sphinxAtStartPar
To train the RFE model, we first learn the \(k\) dimensional linear subspace \(U\in\mathbb{R}^{d \times k}\) via POD.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k} \PYG{o}{=} \PYG{l+m+mi}{6}

\PYG{n}{ss} \PYG{o}{=} \PYG{n}{ROM}\PYG{o}{.}\PYG{n}{subspaces}
\PYG{n}{U} \PYG{o}{=} \PYG{n}{ss}\PYG{o}{.}\PYG{n}{POD}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,}\PYG{n}{k}\PYG{p}{)}

\PYG{n}{UTX} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,}\PYG{n}{U}\PYG{p}{)}
\PYG{n}{UTX\PYGZus{}train} \PYG{o}{=} \PYG{n}{UTX}\PYG{p}{[}\PYG{p}{:}\PYG{n}{num\PYGZus{}train}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}
\end{sphinxVerbatim}

\sphinxAtStartPar
Then, we train the RFE surrogate model:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{model} \PYG{o}{=} \PYG{n}{ROM}\PYG{o}{.}\PYG{n}{response\PYGZus{}surfaces}\PYG{o}{.}\PYG{n}{RFE}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}\PYG{p}{[}\PYG{n}{UTX\PYGZus{}train}\PYG{p}{,} \PYG{n}{Y\PYGZus{}train}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Using the trained model, we then regenerate all 150 snapshots:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{X\PYGZus{}curr} \PYG{o}{=} \PYG{p}{[}\PYG{n}{X}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{num\PYGZus{}snapshot} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{UTX\PYGZus{}curr} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{X\PYGZus{}curr}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{U}\PYG{p}{)}
    \PYG{n}{X\PYGZus{}curr}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{UTX\PYGZus{}curr}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{X\PYGZus{}calc} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{n}{X\PYGZus{}curr}\PYG{p}{,}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relative error = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{ROM}\PYG{o}{.}\PYG{n}{utils}\PYG{o}{.}\PYG{n}{rel\PYGZus{}error}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,}\PYG{n}{X\PYGZus{}calc}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
To visualize our generated snapshot at time \sphinxcode{\sphinxupquote{time\_show}}, we display its contour plot:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{time\PYGZus{}show} \PYG{o}{=} \PYG{l+m+mi}{100}

\PYG{n}{levels} \PYG{o}{=} \PYG{l+m+mf}{0.5}
\PYG{n}{color} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{IceFire}\PYG{l+s+s1}{\PYGZsq{}}

\PYG{n}{num\PYGZus{}x} \PYG{o}{=} \PYG{l+m+mi}{199}
\PYG{n}{num\PYGZus{}y} \PYG{o}{=} \PYG{l+m+mi}{449}

\PYG{c+c1}{\PYGZsh{} vorticity}
\PYG{n}{PLOT\PYGZus{}RFE} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{n}{X\PYGZus{}calc}\PYG{p}{)}
\PYG{n}{PLOT\PYGZus{}RFE}\PYG{p}{[}\PYG{n}{PLOT\PYGZus{}RFE}\PYG{o}{\PYGZgt{}}\PYG{l+m+mf}{5.}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mf}{5.}
\PYG{n}{PLOT\PYGZus{}RFE}\PYG{p}{[}\PYG{n}{PLOT\PYGZus{}RFE}\PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{5.}\PYG{p}{]} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{5.}

\PYG{c+c1}{\PYGZsh{} cylinder}
\PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{2}
\PYG{n}{theta} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}
\PYG{n}{x\PYGZus{}cyl} \PYG{o}{=} \PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{theta}\PYG{p}{)}\PYG{p}{)}\PYG{o}{/}\PYG{n}{scale}
\PYG{n}{y\PYGZus{}cyl} \PYG{o}{=} \PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{cos}\PYG{p}{(}\PYG{n}{theta}\PYG{p}{)}\PYG{p}{)}\PYG{o}{/}\PYG{n}{scale}

\PYG{n}{fig} \PYG{o}{=} \PYG{n}{go}\PYG{o}{.}\PYG{n}{Figure}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{fig}\PYG{o}{.}\PYG{n}{add\PYGZus{}trace}\PYG{p}{(}
   \PYG{n}{go}\PYG{o}{.}\PYG{n}{Contour}\PYG{p}{(}
      \PYG{n}{z} \PYG{o}{=} \PYG{n}{PLOT\PYGZus{}RFE}\PYG{p}{[}\PYG{n}{time\PYGZus{}show}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{n}{num\PYGZus{}y}\PYG{p}{,}\PYG{n}{num\PYGZus{}x}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,}
      \PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{8}\PYG{p}{,}\PYG{n}{num\PYGZus{}y}\PYG{p}{)}\PYG{p}{,}
      \PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{num\PYGZus{}x}\PYG{p}{)}\PYG{p}{,}
      \PYG{n}{colorscale} \PYG{o}{=} \PYG{n}{color}\PYG{p}{,}
      \PYG{n}{contours}\PYG{o}{=}\PYG{n+nb}{dict}\PYG{p}{(}
         \PYG{n}{start}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5}\PYG{p}{,}
         \PYG{n}{end}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,}
         \PYG{n}{size}\PYG{o}{=}\PYG{n}{levels}\PYG{p}{,}
         \PYG{p}{)}
      \PYG{p}{)}
   \PYG{p}{)}

\PYG{n}{fig}\PYG{o}{.}\PYG{n}{add\PYGZus{}trace}\PYG{p}{(}
   \PYG{n}{go}\PYG{o}{.}\PYG{n}{Scatter}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{x\PYGZus{}cyl}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{y\PYGZus{}cyl}\PYG{p}{,}
   \PYG{n}{fill}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{toself}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
   \PYG{n}{fillcolor}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
   \PYG{n}{line\PYGZus{}color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
   \PYG{n}{opacity}\PYG{o}{=}\PYG{l+m+mf}{1.0}\PYG{p}{)}
\PYG{p}{)}

\PYG{n}{fig}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{vort_t100}.png}


\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{r}
\item\relax\sphinxstyleindexentry{rom.gradients}\sphinxstyleindexpageref{rom:\detokenize{module-rom.gradients}}
\item\relax\sphinxstyleindexentry{rom.response\_surfaces}\sphinxstyleindexpageref{rom:\detokenize{module-rom.response_surfaces}}
\item\relax\sphinxstyleindexentry{rom.subspaces}\sphinxstyleindexpageref{rom:\detokenize{module-rom.subspaces}}
\item\relax\sphinxstyleindexentry{rom.utils}\sphinxstyleindexpageref{rom:\detokenize{module-rom.utils}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}